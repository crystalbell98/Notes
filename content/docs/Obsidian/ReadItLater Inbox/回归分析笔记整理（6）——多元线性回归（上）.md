[[ReadItLater]] [[Article]]

# [回归分析|笔记整理（6）——多元线性回归（上）](https://zhuanlan.zhihu.com/p/48541799)

**2019.11.18，修改记录：**帽子矩阵已修改。

大家好！

从这一节开始，我们算是正式的进入了**多元线性回归**的内容。在介绍完之前那些困难而复杂的定理之后，这里的内容就会显得稍微平和一些啦，这当然是一件好事ヾ(o◕∀◕)ﾉヾ。

不过，我们这里的内容**也只是一个入门级别**，因为我们下学期专门有一门课会关注多元方面的内容，那才是真正的又一次的深入。不过，相对一元的情况它也不是特别容易，同时会多次涉及到我们之前的内容。不过请放心，**我会在每个地方标注之前的引入**，读者可以通过文章开头的链接索引到之前的文章。

另外，因为回归分析中的符号比较繁杂，因此**一些不必要的上下标我会有所省略**，希望不会对阅读造成太多的负面影响~

提供之前的笔记：

-   [回归分析|笔记整理（1）——引入，一元线性回归（上）](https://zhuanlan.zhihu.com/p/45149297)
-   [回归分析|笔记整理（2）——一元线性回归（下）](https://zhuanlan.zhihu.com/p/45973297)
-   [回归分析|笔记整理（3）——多元正态分布理论（上）](https://zhuanlan.zhihu.com/p/46665398)
-   [回归分析|笔记整理（4）——多元正态分布理论（中）](https://zhuanlan.zhihu.com/p/47122783)
-   [回归分析|笔记整理（5）——多元正态分布理论（下）](https://zhuanlan.zhihu.com/p/47752987)

我们开始本节的内容。

## 目录

-   多元线性回归（上）

-   引入
-   参数估计

-   e 的协差阵
-   \\sigma^2 的无偏估计
-   极大似然估计

-   参数相关性质
-   显著性检验

-   方程显著性检验
-   系数显著性检验

-   拟合优度（上）

-   偏相关系数，偏决定系数

-   中心化，标准化
-   样本相关阵

## 多元线性回归

## 引入

和一元线性回归有相同的定义，多元线性回归中，一个因变量开始由多个自变量来决定，所以它的方程的形式就变成了

> y = $\beta_0$ + $\beta_1$ $x_1$ + $\cdots$ + $\beta_p$ $x_p$ + $\epsilon$  
> E(y) = $\beta_0$ + $\beta_1$ $x_1$ + $\cdots$ + $\beta_p$ $x_p$

其中第二个就是理论回归方程。因为多元线性回归一个观测值就不再是一个标量而是一个向量了，所以可能自变量的观测值就变成了 (1,x\_{11}, \\ldots, x\_{1p}),(1, x\_{21}, \\ldots, x\_{2p}),\\ldots ，而对应的因变量的观测值不变，还是 y\_1,y\_2, \\ldots ，因此我们把这些观测值每一行每一行的叠加起来就成为了一个向量或者矩阵，所以引入矩阵的表示是必要的。

为了方便，记

> y = $\begin{bmatrix}y_1 \\\ y_2 \\\ \vdots \\\ y_n \end{bmatrix}$, X = $\begin{bmatrix}1 & x_{11} & \cdots & x_{1p} \\\ 1 & x_{21} & \cdots & x_{2p} \\\ \vdots & \vdots & &\vdots \\\ 1 & x_{n1} & \cdots &x_{np}\end{bmatrix}$ , $\epsilon = \begin{bmatrix}\epsilon_1 \\\ \epsilon_2 \\\ \vdots \\\ \epsilon_n\end{bmatrix}$, $\beta = \begin{bmatrix}\beta_0 \\\ \beta_1 \\\ \vdots \\\ \beta_p\end{bmatrix}$

那么这个时候的多元线性回归的表示就变成了 y=X$\beta$+ $\epsilon$ ，其中 X 我们一般称为**设计矩阵**。

同样的，它也有自己的一套**基本假定**。对应的下面三条。

> Notation:  
> (1) r(X) = p+1 ，且 $x_1, \ldots, x_p$ 不是随机变量。  
> (2)G-M条件  
> (3) \\epsilon \\sim N(0, \\sigma^2 I\_n)

可能有的人要问了，如何去理解多元线性回归的**含义**？我们其实根据理论回归方程，比方说对自变量 x\_1 求偏导，就会有 \\frac{\\partial E(y)}{\\partial x\_1} = \\beta\_1 ，注意偏导的含义是**控制其余变量不变的**。因此这里 \\beta\_1 的含义就是在**控制其余因素不变的情况下**，我每增加一个单位的 x\_1 会给我的因变量带来 \\beta\_1 个单位的影响。这也是和一元回归稍微不一样的地方。

## 参数估计

逃不掉的还是参数估计，首先第一步就是要得到我们的 \\beta 的估计值。方法和一元的情况完全相同，但是结果为什么是《数理统计概要》中介绍的那个矩阵，可能还是需要看看具体的推导。

注意到我们现在需要最小化的函数是

> Q(\\beta\_0,\\beta\_1,\\ldots,\\beta\_p)=\\sum\_{i=1}^{n}(y\_i-\\beta\_0-\\beta\_1x\_{i1}-\\ldots-\\beta\_px\_{ip})^2

（这里为了方便，我没有给每一个 \\beta\_i 加上帽子，实际要写的时候应该写成 \\hat \\beta\_i ）

那么同样的，对每一个需要估计的参数求偏导，我们可以得到一系列的方程组如下

> \\sum(y\_i-\\beta\_0-\\beta\_1x\_{i1}-\\ldots-\\beta\_px\_{ip})=0  
> \\sum(y\_i-\\beta\_0-\\beta\_1x\_{i1}-\\ldots-\\beta\_px\_{ip})x\_{i1}=0  
> \\cdots  
> \\sum(y\_i-\\beta\_0-\\beta\_1x\_{i1}-\\ldots-\\beta\_px\_{ip})x\_{ip}=0

使用常规的方法确实可以解出这样的方程组，可是这样未免让初学者看着有些太可怕了不是吗？所以我们这里使用**矩阵**的语言来说明，会更容易些。

首先我们来看第一个式子，求和号需要求和的这一个元素是什么？如果我们设 \\hat y\_i = \\beta\_0+\\beta\_1x\_{i1}+\\ldots+\\beta\_px\_{ip} ，那么相当于每一个元素的残差求和为0。也就是说， \\sum e\_i = 0 。关键来了，我们把它写成矩阵形式，也就是 e^T\\mathbf{1}=0 ，这里 e=(e\_1,e\_2,\\ldots,e\_n)^T 。你没有忘记啥是 \\mathbf{1} 吧？

那么同样的，我们看看第二个式子，我需要说的是它是针对下标 \_i 来做的求和，所以如果我们同样的可以得到 \\sum e\_ix\_{i1}=0 ，也就是 e^TX\_1=0 ，其中 X\_1=(x\_{11},x\_{21},\\ldots,x\_{n1})^T 。

以此类推，我们事实上可以得到一系列内积为0的式子。这样的话，我们事实上可以把这些关于 X\_i 的矩阵拼在一起，也就是说 e^T\\begin{bmatrix}\\mathbf{1} & X\_1 & \\cdots & X\_n\\end{bmatrix}=0 。这样的话我们就可以得到我们最终的结论

> e^TX=0

这里 X 就是我们上面的设计矩阵。这就很方便了，因为 e^T=y-X\\hat \\beta ，所以我们代进去做一些化简

> (y-X\\hat \\beta)^TX=0  
> y^TX=\\hat \\beta^TX^TX  
> X^Ty=X^TX\\hat \\beta  
> \\hat \\beta = (X^TX)^{-1}X^Ty

这里要注意矩阵 X 是列满秩的（多元回归基本假定第一条），所以 X^TX 是可逆矩阵，因此运算是合法的。通过这一套计算，我们也最终得到了我们想要的结果。

说到这里，我们要提一下我们所得到的矩阵。因为它太重要了，所以需要给它一个单独的定义。

> Definition 1: hat matrix  
> 定义矩阵 H=X(X^TX)^{-1}X^T 为帽子矩阵。

也就是说，如果我们的观测值是 y （请注意，这里是**向量**不再是数了，请尽快习惯矩阵的语言）。那么估计值就是 Hy 。

这个矩阵在数值线性代数，数值逼近等课程中也是高频出现的，足以显现出其重要性。

## e 的协差阵

一个重要的性质是帽子矩阵是**对称幂等的**（见第三节笔记），因此 tr(H)=r(H)=p+1=\\sum\_{i=1}^nh\_{ii} 。而且有了帽子矩阵，我们就可以把得到的残差写成 e=y-\\hat y=(I-H)y 。也很方便计算它的协差阵。同样的，根据第三节笔记的理论，我们有

> cov(e,e) = cov((I-H)y,(I-H)y)=(I-H)cov(y,y)(I-H)^T  
> \=(I-H)\\sigma^2I\_n(I-H)^T=\\sigma^2(I-H)(I-H)=\\sigma^2(I-H)

（一个重要的性质是如果 H 对称幂等，那么 I-H 也是对称幂等阵。在数值线性代数中它们被称为**投影矩阵**。）

因此，如果把向量的每一元拆开，就可以得到 e\_i=\\sigma^2(1-h\_{ii}) 。其中 h\_{ii} 是帽子矩阵的对角元，也叫**杠杆值**，在之后的章节中还会涉及到。

## \\sigma^2 的无偏估计

在一元线性回归（笔记第二节）中，我们介绍了方差的无偏估计 \\frac{1}{n-2}\\sum\_{i=1}^{n} e\_i^2 。因此在多元中，有理由相信它还是和残差平方和有关，所以我们考虑 e^Te 的期望。先做些变换~

> e^Te=\[(I-H)y\]^T(I-H)y=y^T(I-H)y  
> \=(X \\beta+\\epsilon)^T(I-H)(X\\beta+\\epsilon) \=(\\beta^TX^T+\\epsilon^T)(I-H)(X\\beta+\\epsilon)  
> \=\\beta^TX^T(I-H)X\\beta+\\beta^TX^T(I-H)\\epsilon+\\epsilon^T(I-H)X\\beta+\\epsilon^T(I-H)\\epsilon

分别来看拆出来的这四项，有没有发现第一项和第三项都有元素 (I-H)X ？那么它是什么呢？

> (I-H)X=X-X\[(X^TX)^{-1}X^T\]X=0

Surprise!这个元素是0，所以第一项和第三项都是0了。第二项是第三项的转置，因此也是0，所以最后只需要看看 \\epsilon^T(I-H)\\epsilon 的期望即可。

> E(\\epsilon^T(I-H)\\epsilon)=E(tr(\\epsilon^T(I-H)\\epsilon) （注意中间的元素其实是个数，因此可以取迹）  
> \=tr(E((I-H)\\epsilon\\epsilon^T))=tr((I-H)E(\\epsilon\\epsilon^T)) （注意协差阵）  
> \=\\sigma^2tr(I-H)=\\sigma^2(n-p-1)

所以我们可以看出 E(e^Te)=\\sigma^2(n-p-1) 。而且在多元回归中， e^Te=SSE ，所以最终的无偏估计就是

> \\hat \\sigma^2=\\frac{SSE}{n-p-1}

有没有感觉和一元的结果，其实是一回事呢？就改了一个分母罢了。

## 极大似然估计(MLE)

这就没有任何新鲜的内容了，目的还是一样，使得

> \\ln L=-\\frac n2\\ln(2\\pi)-\\frac n2 \\ln(\\sigma^2)-\\frac1 {2\\sigma^2}(y-X\\beta)^T(y-X\\beta)

最小，因此连分析的过程都和一元的情况一模一样。所以结果就是 \\hat\\sigma\_L^2=\\frac{SSE}{n} 。

## 参数相关性质

和一元的情况一样，多元得到的参数也有很多性质。比如说

> Proposition 1:  
> \\hat \\beta 为 y 的一个线性变换。

太显然了，不是吗？

> Proposition 2:  
> \\hat\\beta 为 \\beta 的无偏估计。

这个也还行，注意到 E(\\hat \\beta)=E((X^TX)^{-1}X^Ty)=(X^TX)^{-1}X^T(X\\beta)=\\beta 即可。

不过，对于 \\hat \\beta 的协差阵（怕大家忘了，就是一元意义下的方差），情况就有点不一样了。

> Proposition 3:  
> cov(\\hat \\beta)=\\sigma^2(X^TX)^{-1}

不过这也不难，注意到 cov(\\hat \\beta)=\\cancel{(X^TX)^{-1}X^T}cov(y)\\cancel{X}(X^TX)^{-1}=\\sigma^2(X^TX)^{-1} 即可。

> Proposition 4:  
> cov(\\hat \\beta,e)=0

这也不是特别难，我们注意下两个统计量的表达式

> cov(\\hat \\beta,e)=cov((X^TX)^{-1}X^Ty,(I-H)y)=(X^TX)^{-1}X^Tcov(y,y)(I-H)  
> \\sigma^2(X^TX)^{-1}X^T(I-H)=0

这是因为 (I-H)X=0 可以推出 X^T(I-H)=0 。

最后还有个很重要的定理，我们单独拉出来说

> Theorem 1: Gauss-Markov Theorem  
> 设 y \\sim N(X\\beta,\\sigma^2I\_n) ，则 \\beta 的任一线性函数 c^T\\beta 的最小方差线性无偏估计(BLUE)为 c^T\\hat \\beta ，其中 c 为常数向量。

我们首先根据方差公式，先求出 D(c^T\\hat \\beta)=c^TD(\\hat \\beta)c=\\sigma^2c^T(X^TX)^{-1}c 。

我们要注意到的是 \\hat \\beta 其实是 \\beta 的无偏估计，那么另一方面注意到 y 又是 \\hat \\beta 的一个线性变换，所以如果找 c^T\\beta 的无偏估计，我们自然是可以考虑取 Y 的分量的任意线性组合，看它结果是什么。所以我们这里设 a^Ty 是 c^T\\beta 的任一无偏估计，那么就会有 E(a^Ty)=a^TX\\beta=c^T\\beta 。那么注意到 \\beta 这里是任意的，所以有 X^Ta=c 。这样的话，我们看看方差的差

> D(a^Ty)-D(c^T\\hat \\beta)=\\sigma^2(a^Ta-c^T(X^TX)^{-1}c)  
> \=\\sigma^2a^T\[I-X(X^TX)^{-1}X^T\]a \\ge 0

注意第二个式子要把得到的关系式 c=X^Ta 代进去，然后注意中间的矩阵为对称幂等阵，我们在第三节说过它自然也是一个半正定阵。所以实际上就可以得到差不小于0的结论，这就够了。

## 显著性检验

请注意，在多元回归中显著性检验就不止一种情况了。我们一一来看

## 方程显著性检验

方程显著性检验的意思是：我检验**这个回归方程本身**是否有效。也就是说，如果这个检验发现它不显著，那么这个方程就可以直接放弃了。所以它的原假设是

> H\_0:\\beta\_1=\\beta\_2=\\cdots=\\beta\_p=0

也就是说，只要一个回归系数不为0，就拒绝原假设。因为这相当于是一个整体的假设判定，所以我们需要考虑的自然是 SSR 和 SSE ，因为直观上它们综合使用了所有的数据点的信息，因此优先考虑考虑也正常。

现在来看看 SSR 是什么。我们注意到在多元回归中

> SSR=Y^T\[X(X^TX)^{-1}X^T-\\frac1n\\mathbf{11}^T\]Y

这只需要类比一下一元的情况 SSR=\\sum(\\hat y\_i-\\bar y)^2=\\sum \\hat y\_i^2-n\\bar y^2 ，并且注意到 \\hat y^T\\hat y =\[X(X^TX)^{-1}X^TY\]^TX(X^TX)^{-1}X^TY=Y^T\[X(X^TX)^{-1}X^T\]Y 即可。另一方面，如果我们设 J=X(X^TX)^{-1}X^T-\\frac1n\\mathbf{11}^T ，那么直接验证可以得到它是对称幂等阵，所以秩和迹相同，也就可以得到它服从一个分布 \\sigma^2\\chi^2(r(X)-1,\\lambda\_2) （别忘了因为随机向量的协差阵不是单位阵，所以分布前面是有标准化系数的）。

> **2021.7.26补充**：这里的 J 是幂等阵其实不是很显然，需要观察到一个结论是 \\frac1nX(X^TX)^{-1}X^T\\mathbf{11}^T = \\frac1n \\mathbf 1 \\mathbf 1^T ，以及对称的 \\frac1n \\mathbf{11}^T X(X^TX)^{-1}X^T = \\frac1n \\mathbf 1 \\mathbf 1^T 。因为矩阵对称，所以二者验证一个即可。注意到 \[X(X^TX)^{-1}X^T\]X = X ，说明 X(X^TX)^{-1}X^T 是 X 的一个正交投影。因为根据设定， X 有一列是全 1 向量，所以同样也会有一个结论是 \[X(X^TX)^{-1}X^T\] \\mathbf 1 = \\mathbf 1 。代入就可以了。感谢社区群大佬的帮助。

学过F检验（《数理统计概要》第二节）的话，你应该知道上下分布都应该是 \\chi^2 分布，但是从来没有什么非中心化系数之说对吧？所以下一个问题就是，这里的分布是不是也是中心化的，要使用 SSR 就需要我们证明 \\lambda\_2=0 。在第五节我们知道，这就是需要说明

> \\frac1{\\sigma^2}\\beta^TX^T(I\_n-\\frac1n\\mathbf{11}^T)X\\beta=0

不要傻乎乎的直接去证明了。在做假设检验的时候一定要记住的是**原假设成立的情况下构造统计量**。所以这里我们要把 \\beta\_1=\\beta\_2=\\ldots=\\beta\_n=0 代入。根据这个我们再分块 \\beta=\\begin{bmatrix}\\beta\_0 \\\\ 0\\end{bmatrix}, X=\\begin{bmatrix}\\mathbf{1} & \\tilde X\\end{bmatrix} ，那这个时候， X\\beta=\\beta\_0\\mathbf{1},\\beta^TX^T=\\beta\_0\\mathbf{1}^T （注意 \\beta\_0 是数）。代进去就可以得到

> \\beta^TX^T(I\_n-\\frac1n\\mathbf{11}^T)X\\beta=\\beta\_0^2\\mathbf{1}^T\\mathbf{1}-\\beta\_0^2(\\frac1n\\mathbf{1}^T\\mathbf{1}\\mathbf{1}^T\\mathbf{1})=0

这是因为 \\mathbf{1}^T\\mathbf{1}=n 。所以我们证明了 \\frac{SSR}{\\sigma^2}\\sim \\chi^2(r(X)-1)=\\chi^2(p) 。那么 SSE 的情况其实是类似的，可以证明 \\frac{SSE}{\\sigma^2} \\sim \\chi^2(n-p-1) 。

在第五节中，我们已经证明了 SSR,SSE 是独立的，所以有了这些前置条件，就可以构造F检验了。定义好显著性水平就可以进行假设检验了。

> \\frac{SSR/p}{SSE/(n-p-1)} \\sim F(p,n-p-1)

## 系数显著性检验

如果方程是有效的，有一个问题自然是**到底哪个系数是显著的**？这就是系数显著性检验的作用。所以我们实际上要检验的就是

> H\_{0j}: \\beta\_j=0

既然要检验 \\beta\_j ，我们自然要知道的就是 \\hat \\beta\_j 的均值与方差，因为这样我们就可以根据 \\hat \\beta\_j 的值确定置信区间，进而确定是否显著。

根据无偏性和 cov(\\hat \\beta)=\\sigma^2(X^TX)^{-1} ，就可以知道 \\hat \\beta\_j \\sim N(\\beta\_j, c\_{jj}\\sigma^2) ，其中 c\_{jj} 是 (X^TX)^{-1} 的对角线元素。因此如果要检验，我们只需要考虑构造正态或者t检验。

问题来了， \\sigma^2 我们并不知道，所以只能考虑t检验了。t检验的形式是 \\frac{X}{\\sqrt{Y/N}} ，分子上的标准正态分布很好找就是 \\frac{\\hat \\beta\_j}{\\sqrt{c\_{jj}}\\sigma} （还是那句话，在构造分布时原假设条件要代入）。而注意到 SSE/\\sigma^2 \\sim \\chi^2(n-p-1) ，所以根据 \\hat \\sigma=\\sqrt{\\frac{SSE}{n-p-1}} 再组合就可以得到所要的t检验

> t\_j=\\frac{\\hat \\beta\_j}{\\sqrt{c\_{jj}}\\hat \\sigma}\\sim t(n-p-1)

同样的，你也不难得到**每一个回归系数的置信区间**。分子上减一个 \\beta\_j 就可以得到分布，然后解关于 \\beta\_j 的不等式就好，具体细节可以参考《数理统计概要》第三节。

## 拟合优度（上）

顾名思义，拟合优度就是衡量一个回归做的好不好的指标。这一部分我们分开说的原因是，刚开始会涉及一些小的概念，不会占有太多的篇幅。但之后的内容概念可能会有些理论和复杂的东西。

首先是**决定系数**，没什么好说的，定义为 R^2=\\frac{SSR}{SST} ，对应的一个东西叫作**样本复相关系数**，定义为 R=\\sqrt{\\frac{SSR}{SST}} 。所以这就是看拟合的好不好的一个综合指标。

下面我们来说说**偏相关系数**和**偏决定系数**。这个概念是相对比较“动态”的。我们从偏决定系数看起。

偏决定系数考量的因素是：**在每一次添加自变量后，回归的方程 SSE 下降的程度**。通过这句话也就不难理解为什么它是一个衡量**每一个回归系数**的指标了。具体公式如下

> r^2\_{y1 ;2,3,\\cdots,p}=\\frac{SSE(x\_2,\\ldots,x\_p)-SSE(x\_1,\\ldots,x\_p)}{SSE(x\_2,\\ldots,x\_p)}

注意它的下标，上面的式子其实表示的意思就是：在已经有了第2，3，...，p个自变量后，新添加第1个自变量后， SSE 到底下降了多少。所以要理解为**第1个自变量与其余自变量已经存在的模型的偏决定系数**。

至于偏相关系数，那没什么好说的了，就是偏决定系数开个根号。

实际情况中，有的人希望能够检验出**一下子添加多个变量后会对 SSE 造成多大影响**，Prof提供了一个计算的思路。不过它不是我们要关注的重点，感兴趣的注意下就好。

> \\frac{SST-SSE(x\_1,x\_2)}{SST}=1-\\frac{SSE(x\_1)}{SST}\\frac{SSE(x\_1,x\_2)}{SSE(x\_1)}=1-(1-r\_{y\_1}^2)(1-r\_{y\_2;1}^2)

## 中心化，标准化

这又是多元回归中的一些小的操作。

首先是**中心化**，因为多元回归的直线一定过 (\\bar x\_1,\\bar x\_2,\\ldots,\\bar x\_p,\\bar y) （对离差平方和的 \\beta\_0 求偏导即可得到），所以中心化的意思就是，**把所有的数据点都平移，使得回归的原点变成数据的均值点**。换句话说，如果我们设原始的理论回归方程为

> y\_i=\\beta\_0+x\_{i1}\\beta\_1+\\ldots+x\_{ip}\\beta\_p+\\epsilon\_i, i=1,\\cdots,n

那所要做的变换就是 x\_{ij}'=x\_{ij}-\\bar x\_j, j=1,\\cdots,p 和 y\_i'=y\_i-\\bar y 。

如果做了这样的一个变换，那么如果我们设 \\hat y=\\hat \\beta\_0+\\hat \\beta\_1x\_1+\\cdots+\\hat \\beta\_px\_p 是它的经验回归方程，对右边代入变换就可以得到 \\hat y=\\bar y+\\hat \\beta\_1x\_1'+\\cdots+\\hat \\beta\_px\_p' 。又因为我们对变量 y 也做了类似的变换，所以这样就会有最后的中心化经验回归方程

> \\hat y'=\\hat \\beta\_1x\_1'+\\cdots+\\hat \\beta\_px\_p'

可以看出来，这样的方程是**没有常数项的**。

在做PCA（Principal Component Analysis，主成分分析）的时候，刚开始的数据处理往往都要中心化。不过要注意的是回归中的中心化**会有一些隐患**。

现在我们不对 y 作变换，但是 x\_i 依然中心化，那么相当于针对原来的理论回归方程 y=X\\beta+\\epsilon 两边同乘了一个**中心化矩阵** I\_n-\\frac1n \\mathbf{11}^T 。这样的话，如果我们设中心化之后的数据为 \\tilde X ，那么没有中心化的时候，模型相当于 y=\[\\mathbf{1} \\mid \\tilde X\]\\begin{bmatrix}\\gamma\_0 \\\\ \\beta\_l\\end{bmatrix}+\\epsilon ，两边乘完之后就变成了

> \\tilde y=\[\\mathbf{0} \\mid \\tilde X\]\\begin{bmatrix}\\gamma\_0 \\\\ \\beta\_l\\end{bmatrix}+\\tilde \\epsilon=\\tilde X\\beta\_l + \\tilde \\epsilon

形式一样，但是因为 \\epsilon 被多乘了一个东西，它的**独立同分布条件就不一定能够满足了**。

那么什么是**标准化**呢？其实很简单，就是下面的变换

> x\_{ij}^\*=\\frac{x\_{ij}-\\bar x\_j}{\\sqrt{L\_{jj}}},y\_i^\*=\\frac{y\_i-\\bar y}{\\sqrt{L\_{yy}}},i=1,\\cdots,n

其中 L\_{jj}=\\sum\_{i=1}^{n}(x\_{ij}-\\bar x\_j)^2 。这样变换后，新的回归的离差平方和就是1。

## 样本相关阵

我们用它来结束这一节，把一些较理论的收尾的部分放到下一节。

你应该没有忘记我们在一元回归中介绍的**简单相关系数**。在多元中它的定义是一样的。

> r\_{ij}=\\frac{L\_{ij}}{\\sqrt{L\_{ii}L\_{jj}}}

那么如果我们把每一对自变量之间的关系都拉出来进行衡量，自然会得到一个矩阵，这就是样本相关阵

> r=\\begin{bmatrix}1 & r\_{12} & \\cdots & r\_{1p} \\\\ r\_{21} & 1 & \\cdots & r\_{2p} \\\\ \\vdots & \\vdots&\\ddots& \\vdots \\\\ r\_{p1} & r\_{p2} & \\cdots & 1\\end{bmatrix}

做数据分析的人绝对不陌生，因为你拿到一个dataframe之后，基本上第一件事就是看它每个变量之间的相关性。

实际计算中，如果单独计算每一个 r\_{ij} 也必然是很累人的事情。所以标准化就派上了用场。如果我们设 X^\*=\\begin{bmatrix}\\frac{x\_{11}-\\bar x\_1}{\\sqrt{L\_{11}}} &\\frac{x\_{12}-\\bar x\_2}{\\sqrt{L\_{22}}} &\\cdots &\\frac{x\_{1p}-\\bar x\_p}{\\sqrt{L\_{pp}}} \\\\ \\frac{x\_{21}-\\bar x\_1}{\\sqrt{L\_{11}}} & \\frac{x\_{22}-\\bar x\_2}{\\sqrt{L\_{22}}}& \\cdots & \\frac{x\_{2p}-\\bar x\_p}{\\sqrt{L\_{pp}}} \\\\ \\vdots & \\vdots & \\ddots& \\vdots \\\\ \\frac{x\_{n1}-\\bar x\_1}{\\sqrt{L\_{11}}} & \\frac{x\_{n2}-\\bar x\_2}{\\sqrt{L\_{22}}} & \\cdots & \\frac{x\_{np}-\\bar x\_p}{\\sqrt{L\_{pp}}}\\end{bmatrix} ，那么这个时候直接可以验证 r=(X^\*)^TX^\* 。所以标准化之后的矩阵可以直接拿来计算相关阵。

如果我们把矩阵再扩大一下，把因变量 y 也加进去，那么就会有

> r=\\begin{bmatrix}1 & r\_{y1} & r\_{y2} & \\cdots & r\_{yp} \\\\ r\_{1y} & 1 & r\_{12} &\\cdots & r\_{2p} \\\\ \\vdots & \\vdots& \\vdots & \\ddots& \\vdots \\\\ r\_{py} & r\_{p1} & \\cdots & r\_{p,p-1} & 1\\end{bmatrix}

实际研究中，增广阵会用在高维数据（**自变量的个数大于或者接近数据的个数**）的处理中。很多时候我们都会先考虑每一个变量与因变量之间的大小关系，进而筛掉部分变量，以使得回归可以正常进行。

我们还没有完全结束多元回归，还剩一些比较理论的东西。不过这一节的东西再加感觉难度上又不会友好了，就先到此为止吧。

## 小结

本节我们主要关注了多元回归的大部分内容。细心的读者可以发现大部分多元回归的分析思路（参数估计，假设检验）都与一元回归完全相同。而诸如相关系数这样的概念很多地方也和一元的情况相重合。所以在多元中，其实大部分内容都可以类比笔记的第1-2节一元回归的部分去对比学习，这会比较有帮助。

因为我们下一周就要进行《回归分析》的期中考试，所以我们近期还会推出一篇文章，这样之前的文章就覆盖了我们学院的期中考试的所有内容。也请大家稍作等待，文章很快就好~
