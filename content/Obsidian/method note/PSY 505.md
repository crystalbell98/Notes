# Class Zoom Link 
[https://stonybrook.zoom.us/j/92823434355?pwd=NDU4U2lkdXp1U3dhOWNBSkxDYVJzUT09](https://stonybrook.zoom.us/j/92823434355?pwd=NDU4U2lkdXp1U3dhOWNBSkxDYVJzUT09)

# PCA: repackage information
- PCA using Singular Value Decomposition (SVD)
	- ![[Pasted image 20230925135119.png|300]] ![[Pasted image 20230925135212.png|300]] ![[Pasted image 20230925135257.png|200]] ![[Pasted image 20230925135403.png|500]] ![[Pasted image 20230925135513.png|500]] ![[Pasted image 20230925135746.png|500]]  ![[Pasted image 20230925140109.png|500]] 
- **Why PCA**
	1. Capture the True Signal
		- One item has (shared variance, unique variance, error)
		- get true score without (error and unique part of the item)
		- but if the measurement is bad at the beginning, factor analysis cannot fix it.
	2. Multicollinearity
- PCA goal is to *maximize the variance* accounted for. principal components analysis is looking basically only at that *diagonal* and trying to find the best way to account for the variance on that diagonal
- **Correlation Coefficients Matrix**: Symmetric square
	- it always has *one's on this main diagonal* as because *all* the variables are *standardized* or variances and standard deviations are standardized. So we have these ones on the diagonal.
- **Variance Covariance Matrix**: Not standardized, PCA probably will count more on large variance variable. In PCA, SEM, we can choose correlation matrix or covariance matrix. But in EFA, we can only do correlation
- For PCA,  recommend that you do it both ways (correlation and covariance). At least you won't be missing anything. Then you have the problem of, you know, a lot of information you have. Figure out what to do with it, but it's much better to have more information and have to figure out what to do with it than to have blinders
- **Eigenvalues and Eigenvectors**
	- The eigenvalue λ: a number. Eigenvalues are just measures of variation. 
	- Eigenvector indicate the ==direction== while eigenvalue is the ==magnitude== (or geometric length)
	- Eigenvectors are orthogonal to each other, not related to each other
	- *Pairs of* eigenvalue and eigenvector. **Eigen decomposition**
	- *The trace of matrix=sum of the main diagonal.* If we standardize the variable the trace = number of variables. if we do not standardize them, the trace=total variance of set variables
	-   ![[Pasted image 20230925142123.png|200]] ![[Pasted image 20230925142235.png|240]]  
		- ![[Pasted image 20231025131414.png|500]] ![[Pasted image 20231025131430.png|500]] 
		- This highlights that there is something very unique and important about the relationship of each eigenvalue-eigenvector pair as well as the relationship of the eigenvalues/vectors and the (co)variances of the matrices they characterize.
			- variance matrix for PCA, covariance matrix for EFA
		- each time you are predicting something and looking at the residuals in the next step.
		- The eigenvalues are in a descending order. Because again, we're residualizing, and looking at what's left in our data matrix after we pull out each sequential eigenvalue.
	- ![[Pasted image 20231011145708.png|300]] ![[Pasted image 20231011150150.png|300]] 
	- matrix and multiplying it by its transpose, gives you the *sum of the squares and cross products*. V'V=I means eigenvectors are orthogonal to each other 
		- ![[Pasted image 20231025230253.png|600]] 
	-  The *eigenvector is itself a set of weights*. The weights are used to create new scores for each of the dimensions.
		- ![[Pasted image 20231025231857.png|600]] 
	- ![[Pasted image 20231011152054.png|300]] 
	- note that with the eigenvalues in the latter case taking a value of 1, and each variable in the correlation matrix **B** having a variance/SD of 1. As a general rule, you should not extract any eigenvalues that ==are less than or equal to 1==. You actually might be extracting eigenvalues that are accounting for less information than your original variables did. extracting those eigenvalues has the consequence of **dimension expansion**. 
		- ![[Pasted image 20231025161611.png|300]] 
	- **PCA analyses Examples**
		- ![[Pasted image 20231011152203.png|400]] ![[Pasted image 20231011152234.png|370]]  ![[Pasted image 20231011152301.png|400]] ![[Pasted image 20231011152932.png|400]]  
		- So each variable X and Y contributes to the estimation of principal component one equally.
		- ![[Pasted image 20231025233124.png|400]] ![[Pasted image 20231011154955.png|400]] 
	- Model Identification
		- minimum 3 variables to do factor analysis
			- For 3 variables on factor model: we will estimate 3 loadings 3 residuals which is 6 piece information, an we only have 6 piece information which is just identified model.
- Diagram of regression, PCA, EFA
	- Linear Regression
		- ![[Pasted image 20231025161901.png|300]] 
	- PCA
		-  ![[Pasted image 20231025162400.png|800]]  
	- EFA
		- ![[Pasted image 20231025162643.png|400]] 
		- Variance of x1: variance shared, uniqueness, other shared factor

reliability =  signal /(signal + noise), refers to consistency, it is the lower bound on validity. 
validity:  refers to the accuracy of a measurement, meaning the extent to which a test measures what it claims to measure
valid must be reliable, but reliable not necessary valid

## EFA
- Loadings: multiplying each eigenvector(direction) by the square root of its eigenvalue (strength)
	- loadings are just the way that we can connect the original data to our new solution
- PCA only has one solution while **EFA** can have *many results* based on **rotation**. 
	- Simple structure means that you want big loadings where there are salient loadings. You want everything else that's not a big loading to be 0 really close to 0 as possible. And you don't want there to be cross loadings. You don't want one item to load on more than one factor, if possible, because the more of that stuff you have going on kind of trivial loadings are like middling, or the like. One item loads on 3 factors makes it really hard to interpret what's going on.
	- ![[Pasted image 20231026111803.png|400]] 
- Difference with PCA
	- every factor is standardized. because identification. already has a lot of loadings and covariance, error
	- Using correlation matrix (R)
	- Use R2 in the main diagonal, PCA use variance. EFA: covariance/R2. How much variance in the in the first variable is accounted for by all the others, and the second is account or all the others.
	- can't do EFA with independent matrix, PCA can
	- ![[Pasted image 20231004150304.png|300]] ![[Pasted image 20231025142903.png|400]] 



![[Pasted image 20231101143838.png]]

# Factor Analysis
- Path analysis: Causal analysis with multiple DVs and IVs.
- SEM is the combination of Path Analysis and Factor Analysis
- The latent variable is an approximation of the construct of interest. The basic assumption of latent variable modeling is that variables correlate with one another due to their mutual relationships with one or more latent (unobserved) variables
- The item is saturated by the latent variables
- Classic test theory CTT
	- Slides
		- ![[Pasted image 20231115145407.png|500]] ![[Pasted image 20231115145734.png|400]] ![[Pasted image 20231115145905.png|410]] ![[Pasted image 20231115152244.png|400]] ![[Pasted image 20231115153235.png|400]] 
		- ![[Pasted image 20231115153849.png|400]] ![[Pasted image 20231115154419.png|400]] ![[Pasted image 20231115154942.png|400]] 
		- ![[Pasted image 20231127152319.png|400]]
	- longer questionnaire has better measurements.
		- ![[Pasted image 20231127150818.png|400]]
		- A test to tell you how many items you need to achieve the reliability of x
			- This require all items are *same saturated* by the true score
			- True *score variance* increased *faster* than error variance as you add more items
				- This score variance differentiate people. which is our interest.
			- Psychology require .7 reliability
	- reliability is the property of the measurement. Validity is the property of the theory, the inference you make by your measure. Validity is not stable, it is sample dependent, investigator dependent.
	- Reliability is thus the proportion of variance in the test scores that we could explain if we knew the true score. (Its *square root is the correlation* between T and X)
	- ![[Pasted image 20231127153124.png|400]] ![[Pasted image 20231127153216.png|400]] ![[Pasted image 20231127153542.png|400]]  
- IRT
	- ![[Pasted image 20231115152138.png|400]] 

# Logistic Regression
- Why not linear
	1. not normally
	2. heterosecity, varaince change
	3. out of bond theory
- sigmoidal curve

![[Pasted image 20231023145410.png|500]]

# Poisson
- Binomial is the number of success. Poisson is the count of event. What’s the differences of those two distributions?
	- ![[Pasted image 20231106152343.png|500]] 
	- Binomial is number over the all numbers of Bernoulli trials. The Poisson is the number over the number of Benoulli trials within the time period.
# Qualitative Study
Q: Do qualitative findings require validation? What does this process entail? Does it involve dividing the data into training and testing sets?
A: qual work is more about understanding the process /“why” i think as opposed to generalizability

Q: why need so many new measurements for same construct
A: 1. Particular population 2. include minor people (women, color). 3 construct profanation, all theory never died but just fade away and re-created like new by different people 

All models are wrong but some are useful!


# DataCamp ChatGPT
## ChatGPT
 ChatGPT is far more generalizable, as it uses its understanding of language to interpret the question or task and determine the most appropriate response. The ability for ChatGPT to remember the conversation history and for the user to provide follow-up corrections to responses are two extremely powerful capabilities of the model.
- Creating marketing content. This tweet is pretty good: ChatGPT came up with a catchy slogan (Data is power), considered Twitter's character limitations, and utilized hashtags and emojis to be relevant and engaging for the audience. ChatGPT is already being applied to streamline many different marketing tasks, including *creating email templates, writing blog post titles and descriptions, and copyediting large bodies of text*.
- Keep a conversation to one topic and create new conversations for different topics
 ![[Pasted image 20231025105317.png|200]] ![[Pasted image 20231025105422.png|300]]  
 - How does ChatGPT interpret a prompt
	-  ![[Pasted image 20231025111144.png|300]] ![[Pasted image 20231025111203.png|300]] 
	- More context could be added to the prompt for greater personalization, such as key role-specific skills and company culture information. Let's use this understanding to begin thinking about 
- best practices for writing prompts.
	- Be clear and specific
		- specify the desired length in a summarizing task
	- Keep it concise
		- remove un-useful context
	- Use correct grammar and spelling
	- Provide examples
		- ![[Pasted image 20231025111621.png|300]] 
- Use
	- ![[Pasted image 20231025112810.png|300]] ![[Pasted image 20231025112830.png|300]] ![[Pasted image 20231025112905.png|300]] ![[Pasted image 20231025112936.png|300]] 
	- ![[Pasted image 20231025113511.png|400]] ![[Pasted image 20231025113611.png|400]] 
- Ownership
	- ![[Pasted image 20231025114153.png|400]] ![[Pasted image 20231025114241.png|400]] ![[Pasted image 20231025114347.png|400]] ![[Pasted image 20231025114417.png|400]] 
	- ![[Pasted image 20231025114627.png|500]] 
- Improvement
	- ![[Pasted image 20231025115001.png|600]] 


# Network Analysis
[link](file:///Users/yuan/Dropbox/papers/cog%20trajectory%20in%20SZ/Eaton%20A%20review%20of%20approaches%20and%20models%20in%20psychopathology%20conceptualization%20research%20FINAL.pdf)